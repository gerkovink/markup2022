%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Nina van Gerwen at 2022-10-05 11:30:32 +0200 


%% Saved with string encoding Unicode (UTF-8) 



@article{dataset,
	author = {Darrell Bock, R and Lieberman, Marcus},
	date-added = {2022-10-05 11:30:21 +0200},
	date-modified = {2022-10-05 11:30:30 +0200},
	journal = {Psychometrika},
	number = {2},
	pages = {179--197},
	publisher = {Springer},
	title = {Fitting a response model forn dichotomously scored items},
	volume = {35},
	year = {1970}}

@article{consq3,
	abstract = { In this article, the practical consequences of violations of unidimensionality on selection decisions in the framework of unidimensional item response theory (IRT) models are investigated based on simulated data. The factors manipulated include the severity of violations, the proportion of misfitting items, and test length. The outcomes that were considered are the precision and accuracy of the estimated model parameters, the correlations of estimated ability (θ^) and number-correct (NC) scores with the true ability (θ), the ranks of the examinees and the overlap between sets of examinees selected based on either θ, θ^, or NC scores, and the bias in criterion-related validity estimates. Results show that the θ^ values were unbiased by violations of unidimensionality, but their precision decreased as multidimensionality and the proportion of misfitting items increased; the estimated item parameters were robust to violations of unidimensionality. The correlations between θ, θ^, and NC scores, the agreement between the three selection criteria, and the accuracy of criterion-related validity estimates are all negatively affected, to some extent, by increasing levels of multidimensionality and the proportion of misfitting items. However, removing the misfitting items only improved the results in the case of severe multidimensionality and large proportion of misfitting items, and deteriorated them otherwise. },
	author = {Daniela R. Cri{\c s}an and Jorge N. Tendeiro and Rob R. Meijer},
	date-added = {2022-09-28 13:36:17 +0200},
	date-modified = {2022-09-28 13:36:24 +0200},
	doi = {10.1177/0146621617695522},
	eprint = {https://doi.org/10.1177/0146621617695522},
	journal = {Applied Psychological Measurement},
	note = {PMID: 28804181},
	number = {6},
	pages = {439-455},
	title = {Investigating the Practical Consequences of Model Misfit in Unidimensional IRT Models},
	url = {https://doi.org/10.1177/0146621617695522},
	volume = {41},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1177/0146621617695522}}

@inproceedings{consq2,
	author = {Jiao, Hong and Lau, Allen C},
	booktitle = {Annual meeting of the National Council of Educational Measurement, Chicago, IL},
	date-added = {2022-09-28 13:35:04 +0200},
	date-modified = {2022-09-28 13:35:12 +0200},
	title = {The effects of model misfit in computerized classification test},
	year = {2003}}

@article{cfi,
	author = {Bentler, Peter M},
	journal = {Psychological bulletin},
	number = {2},
	pages = {238},
	publisher = {American Psychological Association},
	title = {Comparative fit indexes in structural models.},
	volume = {107},
	year = {1990}}

@article{tli,
	author = {Tucker, Ledyard R and Lewis, Charles},
	journal = {Psychometrika},
	number = {1},
	pages = {1--10},
	publisher = {Springer},
	title = {A reliability coefficient for maximum likelihood factor analysis},
	volume = {38},
	year = {1973}}

@article{4plconsist2,
	author = {Loken, Eric and Rulison, Kelly L},
	journal = {British Journal of Mathematical and Statistical Psychology},
	number = {3},
	pages = {509--525},
	publisher = {Wiley Online Library},
	title = {Estimation of a four-parameter item response theory model},
	volume = {63},
	year = {2010}}

@article{4plconsist1,
	author = {Barton, Mark A and Lord, Frederic M},
	journal = {ETS Research Report Series},
	number = {1},
	pages = {i--8},
	publisher = {Wiley Online Library},
	title = {An upper asymptote for the three-parameter logistic item-response model},
	volume = {1981},
	year = {1981}}

@phdthesis{yangfitindex,
	author = {Yang, Xiaotong},
	school = {Florida State University, College of Education},
	title = {Comparing Global Model-Data Fit Indices in Item Response Theory Applications},
	type = {{PhD} dissertation},
	year = {2020}}

@article{tliirt,
	author = {Cai, Li and Chung, Seung Won and Lee, Taehun},
	journal = {Prevention Science},
	pages = {1--12},
	publisher = {Springer},
	title = {Incremental Model Fit Assessment in the Case of Categorical Data: Tucker--Lewis Index for Item Response Theory Modeling},
	year = {2021}}

@article{chi2sens,
	abstract = {Monte Carlo computer simulations were used to investigate the performance of three χ2 test statistics in confirmatory factor analysis (CFA). Normal theory maximum likelihood χ2 (ML), Browne's asymptotic distribution free χ2 (ADF), and the Satorra-Bentler rescaled χ2 (SB) were examined under varying conditions of sample size, model specification, and multivariate distribution. For properly specified models, ML and SB showed no evidence of bias under normal distributions across all sample sizes, whereas ADF was biased at all but the largest sample sizes. ML was increasingly overestimated with increasing nonnormality, but both SB (at all sample sizes) and ADF (only at large sample sizes) showed no evidence of bias. For misspecified models, ML was again inflated with increasing nonnormality, but both SB and ADF were underestimated with increasing nonnormality. It appears that the power of the SB and ADF test statistics to detect a model misspecification is attenuated given nonnormally distributed data.},
	author = {Curran, {Patrick J.} and Stephen West and Finch, {John F.}},
	date-added = {2022-09-22 10:21:55 +0200},
	date-modified = {2022-09-22 10:22:00 +0200},
	doi = {10.1037/1082-989X.1.1.16},
	issn = {1082-989X},
	journal = {Psychological Methods},
	language = {English (US)},
	month = mar,
	number = {1},
	pages = {16--29},
	publisher = {American Psychological Association Inc.},
	title = {The Robustness of Test Statistics to Nonnormality and Specification Error in Confirmatory Factor Analysis},
	volume = {1},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1037/1082-989X.1.1.16}}

@article{rmsea2,
	author = {Maydeu-Olivares, Alberto},
	date-added = {2022-09-22 10:17:29 +0200},
	date-modified = {2022-09-22 10:17:29 +0200},
	doi = {10.1080/15366367.2013.831680},
	journal = {Measurement Interdisciplinary Research and Perspectives},
	month = {07},
	pages = {71-101},
	title = {Goodness-of-Fit Assessment of Item Response Theory Models},
	volume = {11},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1080/15366367.2013.831680}}

@article{rmsea1,
	author = {Maydeu-Olivares, Alberto and Joe, Harry},
	date-added = {2022-09-22 10:16:41 +0200},
	date-modified = {2022-09-22 10:17:37 +0200},
	doi = {10.1080/00273171.2014.911075},
	journal = {Multivariate Behavioral Research},
	month = {07},
	title = {Assessing Approximate Fit in Categorical Data Analysis},
	volume = {49},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1080/00273171.2014.911075}}

@article{dichotirtfit2,
	abstract = { Fit of the model to the data is important if the benefits of item response theory (IRT) are to be obtained. In this study, the authors compared model selection results using the likelihood ratio test, two information-based criteria, and two Bayesian methods. An example illustrated the potential for inconsistency in model selection depending on which of the indices was used. Results from a simulation study indicated that the inconsistencies among the indices were common but that model selection was relatively accurate for longer tests administered to larger sample of examinees. The cross-validation log-likelihood (CVLL) appeared to work the best of the five models for the conditions simulated in this study. },
	author = {Taehoon Kang and Allan S. Cohen},
	date-added = {2022-09-22 10:15:06 +0200},
	date-modified = {2022-09-22 10:15:11 +0200},
	doi = {10.1177/0146621606292213},
	eprint = {https://doi.org/10.1177/0146621606292213},
	journal = {Applied Psychological Measurement},
	number = {4},
	pages = {331-358},
	title = {IRT Model Selection Methods for Dichotomous Items},
	url = {https://doi.org/10.1177/0146621606292213},
	volume = {31},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1177/0146621606292213}}

@article{dichotirtfit,
	abstract = { New goodness-of-fit indices are introduced for dichotomous item response theory (IRT) models. These indices are based on the likelihoods of number-correct scores derived from the IRT model, and they provide a direct comparison of the modeled and observed frequencies for correct and incorrect responses for each number-correct score. The behavior of Pearson's X2 (S-X2) and the likelihood ratio G2 (S-G2) was assessed in a simulation study and compared with two fit indices similar to those currently in use (Q1-X2 and Q1-G2). The simulations included three conditions in which the simulating and fitting models were identical and three conditions involving model misspecification. S-X2 performed well, with Type I error rates close to the expected .05 and .01 levels. Performance of this index improved with increased test length. S-G2 tended to reject the null hypothesis too often, as did Q1-X2 and Q1-G2. The power of S-X2 appeared to be similar for all test lengths, but varied depending on the type of model misspecification. },
	author = {Maria Orlando and David Thissen},
	date-added = {2022-09-22 10:14:18 +0200},
	date-modified = {2022-09-22 10:14:29 +0200},
	doi = {10.1177/01466216000241003},
	eprint = {https://doi.org/10.1177/01466216000241003},
	journal = {Applied Psychological Measurement},
	number = {1},
	pages = {50-64},
	title = {Likelihood-Based Item-Fit Indices for Dichotomous Item Response Theory Models},
	url = {https://doi.org/10.1177/01466216000241003},
	volume = {24},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1177/01466216000241003}}

@article{ltmpack,
	author = {Dimitris Rizopoulos},
	date-added = {2022-09-21 12:49:41 +0200},
	date-modified = {2022-09-21 12:49:51 +0200},
	journal = {Journal of Statistical Software},
	number = {5},
	pages = {1--25},
	title = {ltm: An R package for Latent Variable Modelling and Item Response Theory Analyses},
	url = {https://doi.org/10.18637/jss.v017.i05},
	volume = {17},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.18637/jss.v017.i05}}

@manual{Rstudio,
	address = {Boston, MA},
	author = {RStudio Team},
	date-added = {2022-09-14 14:39:54 +0200},
	date-modified = {2022-09-14 14:41:28 +0200},
	organization = {RStudio, PBC.},
	title = {RStudio: Integrated Development Environment for R},
	url = {http://www.rstudio.com/},
	year = {2020},
	bdsk-url-1 = {http://www.rstudio.com/}}

@article{lavaan,
	author = {Yves Rosseel},
	date-added = {2022-09-14 14:38:13 +0200},
	date-modified = {2022-09-14 14:38:20 +0200},
	doi = {10.18637/jss.v048.i02},
	journal = {Journal of Statistical Software},
	number = {2},
	pages = {1--36},
	title = {{lavaan}: An {R} Package for Structural Equation Modeling},
	volume = {48},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.18637/jss.v048.i02}}

@manual{R,
	address = {Vienna, Austria},
	author = {R Core Team},
	date-added = {2022-09-14 14:34:34 +0200},
	date-modified = {2022-09-14 14:39:47 +0200},
	organization = {R Foundation for Statistical Computing},
	title = {R: A Language and Environmental for Statistical Computing},
	url = {https://www.R-project.org/},
	year = {2021},
	bdsk-url-1 = {https://www.R-project.org/}}

@book{mass,
	address = {New York},
	author = {W. N. Venables and B. D. Ripley},
	date-added = {2022-09-14 14:29:59 +0200},
	date-modified = {2022-09-14 14:30:07 +0200},
	edition = {Fourth},
	note = {ISBN 0-387-95457-0},
	publisher = {Springer},
	title = {Modern Applied Statistics with S},
	url = {https://www.stats.ox.ac.uk/pub/MASS4/},
	year = {2002},
	bdsk-url-1 = {https://www.stats.ox.ac.uk/pub/MASS4/}}

@article{wilkth,
	author = {S. S. Wilks},
	date-added = {2022-09-14 14:22:11 +0200},
	date-modified = {2022-09-14 14:22:26 +0200},
	doi = {10.1214/aoms/1177732360},
	journal = {The Annals of Mathematical Statistics},
	number = {1},
	pages = {60 -- 62},
	publisher = {Institute of Mathematical Statistics},
	title = {{The Large-Sample Distribution of the Likelihood Ratio for Testing Composite Hypotheses}},
	url = {https://doi.org/10.1214/aoms/1177732360},
	volume = {9},
	year = {1938},
	bdsk-url-1 = {https://doi.org/10.1214/aoms/1177732360}}

@article{consq1,
	author = {Zhao, Yue and Hambleton, Ronald},
	date-added = {2022-09-14 13:30:08 +0200},
	date-modified = {2022-09-14 13:30:23 +0200},
	doi = {10.3389/fpsyg.2017.00484},
	journal = {Frontiers in Psychology},
	month = {03},
	title = {Practical Consequences of Item Response Theory Model Misfit in the Context of Test Equating with Mixed-Format Test Data},
	volume = {8},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.3389/fpsyg.2017.00484}}

@article{ref1,
	author = {Christopher D. Nye and Seang-Hwane Joo and Bo Zhang and Stephen Stark},
	doi = {10.1177/1094428119833158},
	eprint = {https://doi.org/10.1177/1094428119833158},
	journal = {Organizational Research Methods},
	number = {3},
	pages = {457-486},
	title = {Advancing and Evaluating IRT Model Data Fit Indices in Organizational Research},
	url = {https://doi.org/10.1177/1094428119833158},
	volume = {23},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1177/1094428119833158}}

@article{ref2,
	author = {Krammer, Georg},
	doi = {10.22237/jmasm/1555594442},
	journal = {Journal of modern applied statistical methods: JMASM},
	month = {11},
	pages = {eP2685},
	title = {The Andersen Likelihood Ratio Test with a Random Split Criterion Lacks Power},
	volume = {17},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.22237/jmasm/1555594442}}

@article{ethfit,
  title={The Ethical Use of Fit Indices in Structural Equation Modeling: Recommendations for Psychologists},
  author={Stone, Bryant M},
  journal={Frontiers in Psychology},
  volume={12},
  year={2021},
  publisher={Frontiers Media SA}
}

@article{cttirt1,
  title={Comparison of classical test theory and item response theory and their applications to test development},
  author={Hambleton, Ronald K and Jones, Russell W},
  journal={Educational measurement: issues and practice},
  volume={12},
  number={3},
  pages={38--47},
  year={1993}
}

@article{YQ,
  title={Using simulation results to choose a latent trait model},
  author={Yen, Wendy M},
  journal={Applied Psychological Measurement},
  volume={5},
  number={2},
  pages={245--262},
  year={1981},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

@article{RMSEAn,
  title={Assessing approximate fit in categorical data analysis},
  author={Maydeu-Olivares, Alberto and Joe, Harry},
  journal={Multivariate Behavioral Research},
  volume={49},
  number={4},
  pages={305--328},
  year={2014},
  publisher={Taylor \& Francis}
}